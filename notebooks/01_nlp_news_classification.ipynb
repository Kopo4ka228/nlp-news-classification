{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddfd7f0",
   "metadata": {},
   "source": [
    "# Проект: Классификация новостных статей по тематикам (NLP)\n",
    "\n",
    "**Автор:** Андрамонов Арсений\n",
    "**Цель:** Разработать модель машинного обучения для автоматического определения темы новостной статьи на основе ее текста. Проект демонстрирует навыки работы с неструктурированными текстовыми данными (NLP).\n",
    "\n",
    "**План работы:**\n",
    "1.  **Загрузка данных:** Используем встроенный в `scikit-learn` датасет **20 Newsgroups**, содержащий ~18,000 статей.\n",
    "2.  **Предобработка текста (Text Preprocessing):**\n",
    "    *   Приведение к нижнему регистру, удаление знаков препинания и цифр.\n",
    "    *   Токенизация (разделение текста на слова).\n",
    "    *   Удаление неинформативных \"стоп-слов\".\n",
    "    *   Лемматизация (приведение слов к их словарной форме).\n",
    "3.  **Векторизация текста:**\n",
    "    *   Преобразование очищенных текстов в числовые векторы с помощью метода **TF-IDF**.\n",
    "4.  **Построение и оценка модели:**\n",
    "    *   Разделение данных на обучающую и тестовую выборки.\n",
    "    *   Обучение модели **Логистическая регрессия**.\n",
    "    *   Оценка качества модели с помощью метрики **Accuracy** и детального отчета по классам.\n",
    "5.  **Заключение:** Формулирование итоговых выводов по проекту."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217322f",
   "metadata": {},
   "source": [
    "## Шаг 1: Загрузка данных из scikit-learn\n",
    "\n",
    "Для этого проекта мы будем использовать классический NLP-датасет \"20 Newsgroups\". Его преимущество в том, что он встроен напрямую в библиотеку `scikit-learn`, что позволяет загрузить его одной командой, без необходимости скачивать CSV-файлы.\n",
    "\n",
    "Датасет состоит из ~18,000 новостных статей, разделенных на 20 различных тематических групп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13662d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки успешно импортированы!\n",
      "\n",
      "Данные успешно загружены!\n",
      "Размер датасета: 18846 строк и 3 столбцов.\n",
      "\n",
      "Распределение статей по категориям (первые 10):\n",
      "target_name\n",
      "comp.windows.x              982\n",
      "rec.sport.hockey            975\n",
      "soc.religion.christian      975\n",
      "rec.motorcycles             969\n",
      "comp.sys.ibm.pc.hardware    964\n",
      "sci.crypt                   962\n",
      "sci.med                     960\n",
      "misc.forsale                959\n",
      "rec.sport.baseball          958\n",
      "sci.electronics             958\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Пример данных, готовых для обработки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AW&amp;ST  had a brief blurb on a Manned Lunar Exp...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>\\nThis question comes up frequently enough tha...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nPhil&gt; Didn't one of the early jet ...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>\"Space Station Redesign Leader Says Cost Goal ...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Original to: keithley@apple.com\\nG'day keithle...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  target_id target_name\n",
       "25   AW&ST  had a brief blurb on a Manned Lunar Exp...         14   sci.space\n",
       "73   \\nThis question comes up frequently enough tha...         14   sci.space\n",
       "75   \\n\\n\\n\\n\\n\\nPhil> Didn't one of the early jet ...         14   sci.space\n",
       "112  \"Space Station Redesign Leader Says Cost Goal ...         14   sci.space\n",
       "158  Original to: keithley@apple.com\\nG'day keithle...         14   sci.space"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Импортируем базовые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ИМПОРТИРУЕМ ФУНКЦИЮ ДЛЯ ЗАГРУЗКИ ДАТАСЕТА\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "print('Библиотеки успешно импортированы!')\n",
    "\n",
    "# 1. ЗАГРУЖАЕМ ДАННЫЕ\n",
    "# Scikit-learn сама скачает и закэширует данные при первом запуске\n",
    "# Мы убираем служебную информацию (headers, footers, quotes), чтобы модель училась только на тексте статей\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(\"\\nДанные успешно загружены!\")\n",
    "\n",
    "# 2. СОЗДАЕМ PANDAS DATAFRAME ДЛЯ УДОБСТВА\n",
    "# Данные загружаются в специальном формате scikit-learn, мы преобразуем их в привычный DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review': newsgroups.data,  # Текст статьи\n",
    "    'target_id': newsgroups.target # ID категории (число от 0 до 19)\n",
    "})\n",
    "\n",
    "# Добавляем названия категорий для наглядности\n",
    "df['target_name'] = df['target_id'].apply(lambda x: newsgroups.target_names[x])\n",
    "\n",
    "print(f\"Размер датасета: {df.shape[0]} строк и {df.shape[1]} столбцов.\")\n",
    "\n",
    "# 3. УДАЛЯЕМ ПУСТЫЕ СТРОКИ, если они есть\n",
    "df.dropna(subset=['review'], inplace=True)\n",
    "df = df[df['review'].str.strip() != '']\n",
    "\n",
    "# 4. ПОСМОТРИМ НА РЕЗУЛЬТАТ\n",
    "print(\"\\nРаспределение статей по категориям (первые 10):\")\n",
    "print(df['target_name'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nПример данных, готовых для обработки:\")\n",
    "# Выведем пример статьи из категории \"Космос\" (sci.space)\n",
    "display(df[df['target_name'] == 'sci.space'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f477ba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Необходимые пакеты NLTK уже загружены.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re # Библиотека для работы с регулярными выражениями\n",
    "\n",
    "# Загружаем необходимые пакеты NLTK\n",
    "# wordnet - база данных для лемматизации\n",
    "# stopwords - список стоп-слов\n",
    "# punkt - токенизатор, который делит текст на предложения/слова\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "    nltk.data.find('corpora/stopwords.zip')\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "    print(\"Необходимые пакеты NLTK уже загружены.\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Загружаем необходимые пакеты NLTK...\")\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    print(\"Пакеты успешно загружены.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Создаем список английских стоп-слов\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Инициализируем лемматизатор\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7aa47",
   "metadata": {},
   "source": [
    "## Шаг 2: Предобработка текста\n",
    "\n",
    "Это самый важный этап в любом NLP-проекте. Чтобы модель могла эффективно работать с текстом, его необходимо \"очистить\" от шума и стандартизировать.\n",
    "\n",
    "Наш пайплайн очистки будет состоять из следующих шагов:\n",
    "1.  **Нормализация:** Приведение текста к нижнему регистру и удаление всех символов, кроме букв.\n",
    "2.  **Токенизация:** Разделение сплошного текста на список отдельных слов (токенов).\n",
    "3.  **Удаление стоп-слов:** Исключение часто встречающихся, но не несущих смысловой нагрузки слов (например, 'a', 'the', 'in').\n",
    "4.  **Лемматизация:** Приведение каждого слова к его начальной, словарной форме (например, 'running', 'ran' -> 'run'). Это помогает модели понять, что разные формы слова несут один и тот же смысл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d9dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Пример работы функции ---\n",
      "\n",
      "ОРИГИНАЛЬНЫЙ ТЕКСТ:\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "\n",
      "ОЧИЩЕННЫЙ ТЕКСТ:\n",
      "sure bashers pen fan pretty confused lack kind post recent pen massacre devil actually bit puzzled bit relieved however going put end nonpittsburghers relief bit praise pen man killing devil worse thought jagr showed much better regular season stats also lot fo fun watch playoff bowman let jagr lot fun next couple game since pen going beat pulp jersey anyway disappointed see islander lose final regular season game pen rule\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Функция для полной предобработки текста.\n",
    "    \"\"\"\n",
    "    # 1. Приведение к нижнему регистру и удаление всего, кроме букв\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A).lower()\n",
    "    \n",
    "    # 2. Токенизация (разбиение на слова)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3. Лемматизация и удаление стоп-слов\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # 4. Соединяем токены обратно в одну строку\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Проверим, как работает наша функция на одном примере\n",
    "sample_text = df['review'].iloc[0]\n",
    "cleaned_text = preprocess_text(sample_text)\n",
    "\n",
    "print(\"--- Пример работы функции ---\")\n",
    "print(\"\\nОРИГИНАЛЬНЫЙ ТЕКСТ:\")\n",
    "print(sample_text)\n",
    "print(\"\\nОЧИЩЕННЫЙ ТЕКСТ:\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e90b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем предобработку всего датасета. Это может занять несколько минут...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f69983a0654af088de1d8e83655c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Предобработка завершена!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>sure bashers pen fan pretty confused lack kind...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>brother market highperformance video card supp...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>finally said dream mediterranean new area grea...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>think scsi card dma transfer disk scsi card dm...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>old jasmine drive use new system understanding...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1  My brother is in the market for a high-perform...   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4  1)    I have an old Jasmine drive which I cann...   \n",
       "\n",
       "                                      review_cleaned               target_name  \n",
       "0  sure bashers pen fan pretty confused lack kind...          rec.sport.hockey  \n",
       "1  brother market highperformance video card supp...  comp.sys.ibm.pc.hardware  \n",
       "2  finally said dream mediterranean new area grea...     talk.politics.mideast  \n",
       "3  think scsi card dma transfer disk scsi card dm...  comp.sys.ibm.pc.hardware  \n",
       "4  old jasmine drive use new system understanding...     comp.sys.mac.hardware  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "# Регистрируем использование tqdm для pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Начинаем предобработку всего датасета. Это может занять несколько минут...\")\n",
    "\n",
    "# Создаем новую колонку с очищенным текстом\n",
    "# .progress_apply() работает как .apply(), но показывает progress bar\n",
    "df['review_cleaned'] = df['review'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\nПредобработка завершена!\")\n",
    "display(df[['review', 'review_cleaned', 'target_name']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04e4cb",
   "metadata": {},
   "source": [
    "## Шаг 3: Векторизация и построение модели\n",
    "\n",
    "После очистки тексты готовы к преобразованию в числовой формат, понятный для алгоритмов машинного обучения.\n",
    "\n",
    "### 3.1. Векторизация с помощью TF-IDF\n",
    "\n",
    "Мы используем метод **TF-IDF (Term Frequency-Inverse Document Frequency)**. Он представляет каждый текст в виде вектора, где вес каждого слова зависит не только от его частоты в данном тексте, но и от его редкости во всем корпусе текстов. Это позволяет выделить ключевые, тематические слова и снизить вес общеупотребительных слов.\n",
    "\n",
    "### 3.2. Обучение и оценка модели\n",
    "\n",
    "В качестве классификатора мы будем использовать **Логистическую регрессию**, так как она является мощной и быстрой базовой моделью для текстовых задач. Качество модели будем оценивать по метрике **Accuracy** (доля правильных ответов) на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e151b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 14664\n",
      "Размер тестовой выборки: 3667\n",
      "------------------------------\n",
      "Тексты преобразованы в матрицу размером: (14664, 5000)\n",
      "------------------------------\n",
      "Начинаем обучение модели (Логистическая регрессия)...\n",
      "Обучение завершено!\n",
      "------------------------------\n",
      "Итоговая точность (Accuracy) на тестовой выборке: 0.6970\n",
      "\n",
      "Детальный отчет о классификации:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.54      0.55      0.55       156\n",
      "           comp.graphics       0.65      0.70      0.68       191\n",
      " comp.os.ms-windows.misc       0.62      0.64      0.63       189\n",
      "comp.sys.ibm.pc.hardware       0.59      0.58      0.58       193\n",
      "   comp.sys.mac.hardware       0.68      0.69      0.68       186\n",
      "          comp.windows.x       0.80      0.78      0.79       197\n",
      "            misc.forsale       0.80      0.66      0.72       192\n",
      "               rec.autos       0.68      0.68      0.68       187\n",
      "         rec.motorcycles       0.66      0.82      0.73       194\n",
      "      rec.sport.baseball       0.81      0.77      0.79       192\n",
      "        rec.sport.hockey       0.88      0.87      0.88       195\n",
      "               sci.crypt       0.88      0.73      0.80       192\n",
      "         sci.electronics       0.61      0.69      0.65       192\n",
      "                 sci.med       0.76      0.77      0.77       192\n",
      "               sci.space       0.76      0.70      0.73       191\n",
      "  soc.religion.christian       0.68      0.71      0.70       195\n",
      "      talk.politics.guns       0.64      0.72      0.68       177\n",
      "   talk.politics.mideast       0.82      0.79      0.80       184\n",
      "      talk.politics.misc       0.57      0.53      0.55       151\n",
      "      talk.religion.misc       0.41      0.36      0.38       121\n",
      "\n",
      "                accuracy                           0.70      3667\n",
      "               macro avg       0.69      0.69      0.69      3667\n",
      "            weighted avg       0.70      0.70      0.70      3667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Определяем признаки (X) и целевую переменную (y)\n",
    "# Используем очищенный текст и числовые ID категорий\n",
    "X = df['review_cleaned']\n",
    "y = df['target_id']\n",
    "\n",
    "# 2. Разделяем данные на обучающую и тестовую выборки (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y # Важно для сохранения баланса классов\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(X_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 3. Векторизация текста\n",
    "# Создаем TF-IDF векторизатор\n",
    "# max_features=5000 говорит, чтобы он использовал только 5000 самых популярных слов. \n",
    "# Это ускоряет обучение и часто улучшает качество, убирая очень редкие слова.\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Обучаем векторизатор на тренировочных данных и преобразуем их в векторы\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Преобразуем тестовые данные, используя уже обученный векторизатор\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Тексты преобразованы в матрицу размером: {X_train_vec.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 4. Обучение модели\n",
    "print(\"Начинаем обучение модели (Логистическая регрессия)...\")\n",
    "# C=10 - параметр регуляризации, подобранный как хороший для этой задачи\n",
    "# max_iter=1000 - увеличиваем количество итераций для сходимости\n",
    "model = LogisticRegression(C=10, max_iter=1000, random_state=42)\n",
    "\n",
    "# Обучаем модель на векторизованных данных\n",
    "model.fit(X_train_vec, y_train)\n",
    "print(\"Обучение завершено!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 5. Оценка качества модели\n",
    "# Делаем предсказания на тестовой выборке\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Считаем точность (Accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Итоговая точность (Accuracy) на тестовой выборке: {accuracy:.4f}\")\n",
    "\n",
    "# Выводим детальный отчет по всем классам\n",
    "# target_names=newsgroups.target_names позволяет видеть названия тем, а не просто цифры\n",
    "print(\"\\nДетальный отчет о классификации:\")\n",
    "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455ec8b",
   "metadata": {},
   "source": [
    "## Шаг 4: Заключение и выводы\n",
    "\n",
    "В ходе проекта была успешно разработана модель для многоклассовой классификации текстов.\n",
    "\n",
    "**Ключевые результаты:**\n",
    "*   Итоговая модель на основе Логистической регрессии и TF-IDF достигла точности **(Accuracy) ~70%** на тестовой выборке при классификации на 20 различных классов. Учитывая, что случайное угадывание дало бы всего 5%, полученный результат является очень хорошим.\n",
    "*   Анализ детального отчета показал, что модель наиболее успешно классифицирует темы с уникальной и специфичной лексикой (например, `rec.sport.hockey`, `sci.crypt`).\n",
    "*   Наибольшие трудности возникают при разделении близких по смыслу и лексическому составу тем (например, разные категории компьютерного \"железа\" или темы, связанные с политикой и религией).\n",
    "\n",
    "Проект демонстрирует владение полным циклом NLP-задачи: от предобработки сырого текста до построения и интерпретации результатов работы модели-классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc34af1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
